

---

# ğŸ­ MoodLens

**MoodLens** is a multi-modal emotion detection system that leverages deep learning to analyze facial expressions, speech patterns, and behavioral trends. It provides real-time emotion recognition through a web-based interface and visualizes emotional patterns over time, making it ideal for research, mental health applications, and human-computer interaction projects.

---

## ğŸ“Œ Description

MoodLens integrates computer vision, audio signal processing, and intelligent aggregation logic to detect and log human emotions. It uses pre-trained neural network models to classify emotions from images, videos, and audio recordings. The platform includes a Flask-based web dashboard for emotion tracking, trend visualization, and analysis.

---

## ğŸš€ Features

- ğŸ§  Facial emotion recognition using CNN models trained on FER-2013 dataset.
- ğŸ™ï¸ Speech emotion classification using MFCC features and deep learning.
- ğŸ¥ Multi-modal video/audio processing and real-time prediction.
- ğŸ“ˆ Trend visualization of emotional states over time.
- ğŸ’¾ Upload interface for image, audio, and video files.
- ğŸ›  Model training, evaluation, and visualization tools included.

---

## ğŸ—‚ï¸ Project Structure

```
MoodLens/
â”œâ”€â”€ app/                          # Flask web app
â”‚   â”œâ”€â”€ static/                   # Static assets (CSS, JS)
â”‚   â”œâ”€â”€ templates/                # HTML templates (Jinja2)
â”‚   â””â”€â”€ __init__.py
â”‚
â”œâ”€â”€ utils/                        # Core logic for emotion detection
â”‚   â”œâ”€â”€ facial_emotion.py
â”‚   â”œâ”€â”€ speech_emotion.py
â”‚   â”œâ”€â”€ emotion_aggregator.py
â”‚   â”œâ”€â”€ emotion_logger.py
â”‚   â”œâ”€â”€ mental_health_logic.py
â”‚   â”œâ”€â”€ video_processing.py
â”‚   â””â”€â”€ test_predict.py
â”‚
â”œâ”€â”€ emotion_model/               # Facial emotion model
â”‚   â””â”€â”€ emotion_model.h5
â”‚
â”œâ”€â”€ uploads/                     # User-uploaded media files
â”œâ”€â”€ speech_emotion_model.h5      # Trained speech emotion model
â”œâ”€â”€ label_encoder.pkl            # Encoded class labels
â”œâ”€â”€ confusion_matrix.png         # Model evaluation image
â”œâ”€â”€ roc_curve.png                # ROC curve visualization
â”œâ”€â”€ model_metrics_summary.csv    # Evaluation metrics summary
â”œâ”€â”€ fer2013/                     # Facial emotion dataset (optional)
â”œâ”€â”€ speech_emotion_dataset/      # Audio emotion dataset
â”œâ”€â”€ train_emotion_model.py       # Script to train facial model
â”œâ”€â”€ train_speech_model.py        # Script to train speech model
â”œâ”€â”€ evaluate_model.py            # Evaluation logic
â”œâ”€â”€ app.py                       # Entry point
â””â”€â”€ requirements.txt             # Project dependencies
```

---

## ğŸ› ï¸ Installation Instructions

1. **Clone the repository**
```bash
git clone https://github.com/your-username/MoodLens.git
cd MoodLens
```

2. **Create and activate a virtual environment**
```bash
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate
```

3. **Install the required dependencies**
```bash
pip install -r requirements.txt
```

---

## â–¶ï¸ How to Start

After installation, run the Flask app:

```bash
python app.py
```

Then visit: [http://localhost:5000](http://localhost:5000)

---

## ğŸ§  Emotion Detection Modules

### ğŸ”¹ Facial Emotion Recognition
- Uses Convolutional Neural Networks.
- Trained on FER-2013 dataset.
- Classes: Angry, Disgust, Fear, Happy, Sad, Surprise, Neutral.

### ğŸ”¹ Speech Emotion Detection
- Extracts MFCC features from audio.
- Uses a trained deep learning model (CNN/LSTM).

### ğŸ”¹ Video and Aggregated Emotion Analysis
- Processes both image and audio frames from uploaded video.
- Aggregates emotions detected from multiple sources to determine dominant mood.

### ğŸ”¹ Mental Health Logic Layer
- Tracks emotional trends.
- Flags prolonged negative states based on temporal data.

---

## ğŸ“Š Evaluation

- **Confusion Matrix**: `confusion_matrix.png`
- **ROC Curve**: `roc_curve.png`
- **Metrics Summary**: `model_metrics_summary.csv`
- **Accuracy and F1-scores**: Available via `evaluate_model.py`

---

## ğŸ“¤ Upload Format

Upload any of the following file types to the `/uploads/` folder or through the web interface:

- ğŸ Video: `.mp4`, `.webm`
- ğŸ¤ Audio: `.wav`
- ğŸ–¼ Images: `.jpeg`, `.jpg`, `.png`

---

## ğŸ·ï¸ Emotion Classes

- ğŸ˜€ Happy  
- ğŸ˜¢ Sad  
- ğŸ˜  Angry  
- ğŸ˜¨ Fear  
- ğŸ¤¢ Disgust  
- ğŸ˜² Surprise  
- ğŸ˜ Neutral

---

## ğŸ“¦ Dependencies

Install with:

```bash
pip install -r requirements.txt
```

Sample contents of `requirements.txt`:
```
tensorflow
keras
flask
numpy
pandas
opencv-python
matplotlib
scikit-learn
librosa
moviepy
```

---

## ğŸ’» Technologies Used

- **Python 3.x**
- **TensorFlow & Keras** â€“ Deep learning models
- **Flask** â€“ Web framework
- **OpenCV** â€“ Image/video processing
- **Librosa** â€“ Audio feature extraction
- **MoviePy** â€“ Video frame/audio handling
- **Scikit-learn** â€“ Evaluation metrics & preprocessing
- **Matplotlib / Seaborn** â€“ Data visualization

---



## ğŸ¤ Contributing

Contributions are welcome! To contribute:

1. Fork the repository
2. Create a new branch (`git checkout -b feature-name`)
3. Commit your changes (`git commit -m 'Added new feature'`)
4. Push to the branch (`git push origin feature-name`)
5. Open a pull request

---

